# -*- coding: utf-8 -*-
"""Queues.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1nQRErb_ElbDaMskbiDhrbD4ODmzFDyS-
"""

!pip install inference supervision

import inference
import supervision as sv

import numpy as np

from inference.models.utils import get_roboflow_model
model = get_roboflow_model(model_id="yolov8n-640")

np.array([[747, 622],[707, 38],[807, 22],[931, 654],[747, 622]])
np.array([[1039, 62],[1243, 546],[1271, 502],[1231, 286],[1107, 34],[1039, 62]])

pip install opencv-python

from cv2 import *
from inference.models.utils import get_roboflow_model
import cv2

model = get_roboflow_model(model_id="yolov8n-640")

video_path = "/content/Retail.mp4"
cap = cv2.VideoCapture(video_path)

import cv2
import numpy as np
import csv
tracker = sv.ByteTrack()

video_path = "/content/Retail.mp4"
cap = cv2.VideoCapture(video_path)

fps = cap.get(cv2.CAP_PROP_FPS)

output_video_path = "output_video.mp4"

#create a VideoWriter object for the output video
frame_width = int(cap.get(3))
frame_height = int(cap.get(4))
fourcc = cv2.VideoWriter_fourcc(*'mp4v')
out = cv2.VideoWriter(output_video_path, fourcc, 20.0, (frame_width, frame_height))

# (Region of Interest)
roi1_coords = np.array([[747, 622],[707, 38],[807, 22],[931, 654],[747, 622]], dtype=np.int32)

roi2_coords = np.array([[1039, 62],[1243, 546],[1271, 502],[1231, 286],[1107, 34],[1039, 62]], dtype=np.int32)

people_enter_queue = {}

timespent = []

filename = "queue_time.csv"

file = open(filename, 'w', newline='')

csv_writer = csv.writer(file)

frame_count =0

while cap.isOpened():

    success, frame = cap.read()

    if success:
        annotated_frame = frame.copy()

        #draw ROIs
        cv2.drawContours(annotated_frame, [roi1_coords], -1, (255, 0, 0), 3)
        cv2.drawContours(annotated_frame, [roi2_coords], -1, (255, 0, 0), 3)


        results = model.infer(frame)
        # print(results[0].predictions)
        detections = sv.Detections.from_inference(results[0])
        detections = tracker.update_with_detections(detections)
        # print(detections)

        # track IDs
        boxes = detections.xyxy
        # print(boxes)

        print("Found: ", people_enter_queue)

        if type(detections.tracker_id) == np.ndarray:
          # track_ids = results[0].boxes.id.int().cpu().tolist()
          track_ids = detections.tracker_id

          # centre of bounding
          for box, track_id in zip(boxes, track_ids):

              print("Tracking:",track_id)

              x1, y1, x2, y2 = box
              x1, y1, x2, y2= int(x1), int(y1), int(x2), int(y2)

              # print(x1,y1,x2,y2)
              x = (x1+x2)/2
              y = (y1+y2)/2

              if ((cv2.pointPolygonTest(roi1_coords, (x, y), False) > 0) or
                (cv2.pointPolygonTest(roi2_coords, (x, y), False) > 0)):

                if str(track_id) not in people_enter_queue:
                    # entry timestamp as an integer when a person enters the queue
                    people_enter_queue[str(track_id)] = int(frame_count)

                # Retrieve the start frame as an integer and calculate time spent in the queue
                start_frame = int(people_enter_queue[str(track_id)])
                time_spent = (frame_count - start_frame) / fps

                # overlay = annotated_frame.copy()

                overlay = annotated_frame.copy()
                cv2.rectangle(overlay, (x1, y1), (x2, y2), (0, 255, 0), -1)

                # overlay with the original frame (to keep transparency effect)
                alpha = 0.4  # Transparency factor
                cv2.addWeighted(overlay, alpha, annotated_frame, 1 - alpha, 0, annotated_frame)
                cv2.putText(annotated_frame,"Person id:"+ str(track_id), (x1,y1-5), cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (20,255,0), 2)
                cv2.putText(annotated_frame, "Time: {:.2f}s".format(time_spent), (x1, y1 - 22),
                  cv2.FONT_HERSHEY_COMPLEX_SMALL, 1, (20, 255, 0), 2)

              else:
                print("outside:",track_id)

                if (str(track_id) in people_enter_queue):
                  #timestamp
                  exit = frame_count

                  #first timestamp
                  start = people_enter_queue[str(track_id)]

                  time_spent = (exit - int(start))/fps
                  print("time spent ", time_spent, "by person", track_id)

                  timespent.append(time_spent)

                  csv_writer.writerow(["Time spent by person "+ str(track_id)+" in line is "+str(time_spent)])

                  people_enter_queue.pop(str(track_id))

        #output video
        out.write(annotated_frame)
        frame_count = frame_count+1

    else:

      #still in line at the end of the video
      for person in people_enter_queue:

        #timestamp
        exit = frame_count

        #first timestamp
        start = people_enter_queue.get(person)

        time_spent = (exit - int(start))/fps
        print("time spent ", time_spent, "by person", person)

        timespent.append(time_spent)

        # Write string to the file
        csv_writer.writerow(["Time spent by person "+ str(person)+" in line is "+str(time_spent)])

      average = sum(timespent)/len(timespent)

      print("Average of list: ", round(average,3))

      csv_writer.writerow(["Average time spent in line is "+str(round(average,3))])

      break

cap.release()
out.release()
cv2.destroyAllWindows()
file.close()

print(f"Output video saved at: {output_video_path}")

file.close()
print(f"Output video saved at: {output_video_path}")

